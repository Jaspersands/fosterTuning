{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Instructions:\n",
        "1. Connect to a runtime with gpu (likely T4 because it's free)\n",
        "2. Drag the Clean Georgia Data.csv into the file bar on the left\n",
        "3. Go to runtime in the top left and press run all\n",
        "\n",
        "\n",
        "Be warned: It's not always 100% accurate"
      ],
      "metadata": {
        "id": "catwN3eprn50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup 5-8 mins"
      ],
      "metadata": {
        "id": "MKKdD7Cwl3iU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6uDVOtYGJURL"
      },
      "outputs": [],
      "source": [
        "!pip install unsloth\n",
        "!pip install peft\n",
        "\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import PeftModel\n",
        "\n",
        "# Load the base model with FastLanguageModel\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length=1048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sklearn\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "oGvHWJS-L0eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Ensure NLTK stopwords are downloaded\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "# Load CSV data for policies\n",
        "file_path = 'Clean Georgia Data.csv'  # Adjust path if necessary\n",
        "df = pd.read_csv(file_path, encoding='MacRoman')\n",
        "\n",
        "# Load the SBERT model for policy ranking\n",
        "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load the main language model and tokenizer with LoRA adapter\n",
        "base_model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
        "adapter_path = \"jaspersands/model\"  # Path to your LoRA adapter on Hugging Face\n",
        "\n",
        "model = PeftModel.from_pretrained(model, adapter_path)"
      ],
      "metadata": {
        "id": "XbPB6_owMDDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "# print(tokenizer)\n",
        "def search_relevant_policies(query, df, top_n=10):\n",
        "    \"\"\"Retrieve top N relevant policies based on cosine similarity with TF-IDF.\"\"\"\n",
        "    tfidf = TfidfVectorizer(stop_words='english')\n",
        "    tfidf_matrix = tfidf.fit_transform(df['Content'])\n",
        "    query_vector = tfidf.transform([query])\n",
        "    cosine_sim = cosine_similarity(query_vector, tfidf_matrix).flatten()\n",
        "    top_indices = cosine_sim.argsort()[-top_n:][::-1]\n",
        "    return df.iloc[top_indices]\n",
        "\n",
        "def process_query(query):\n",
        "    \"\"\"Process a query to return the model's response and the most relevant policy link.\"\"\"\n",
        "\n",
        "    # Step 1: Retrieve relevant policies based on the query\n",
        "    relevant_policies = search_relevant_policies(query, df)\n",
        "\n",
        "    # Step 2: Format the relevant policies for input to the model\n",
        "    formatted_policies = [\n",
        "        f\"Title: {row['Title']}\\nTerritory: {row['Territory']}\\nType: {row['Type']}\\nYear: {row['Year']}\\nCategory: {row['Category']}\\nFrom: {row['From']}\\nTo: {row['To']}\\nContent: {row['Content']}\\nLink: {row['Link to Content']}\\n\"\n",
        "        for _, row in relevant_policies.iterrows()\n",
        "    ]\n",
        "    relevant_policy_text = \"\\n\\n\".join(formatted_policies)\n",
        "\n",
        "    # Step 3: Structure messages for the chat template\n",
        "    messages_with_relevant_policies = [\n",
        "        {\"role\": \"system\", \"content\": relevant_policy_text},\n",
        "        {\"role\": \"user\", \"content\": query},\n",
        "    ]\n",
        "\n",
        "    # Step 4: Tokenize the input for the model\n",
        "    # tokenizer = get_chat_template(\n",
        "    #     tokenizer,\n",
        "    #     chat_template=\"llama-3.1\",\n",
        "    # )\n",
        "    inputs = tokenizer.apply_chat_template(\n",
        "        messages_with_relevant_policies,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Step 5: Generate the model's response\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    outputs = model.generate(input_ids=inputs, max_new_tokens=256, use_cache=True, temperature=1.5, min_p=0.1)\n",
        "    generated_response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Step 6: Rank the relevant policies using SBERT\n",
        "    response_embedding = model_sbert.encode(generated_response, convert_to_tensor=True)\n",
        "    policy_embeddings = model_sbert.encode(relevant_policies['Content'].tolist(), convert_to_tensor=True)\n",
        "    cosine_similarities = util.cos_sim(response_embedding, policy_embeddings).flatten()\n",
        "    most_relevant_index = cosine_similarities.argmax().item()\n",
        "    most_relevant_link = relevant_policies.iloc[most_relevant_index]['Link to Content']\n",
        "\n",
        "    # Step 7: Return the response and the most relevant link\n",
        "    return {\n",
        "        \"response\": generated_response,\n",
        "        \"most_relevant_link\": most_relevant_link\n",
        "    }\n",
        "\n",
        "def get_content_after_query(response_text, query):\n",
        "    # Find the position of the query within the response text\n",
        "    query_position = response_text.lower().find(query.lower())\n",
        "    if query_position != -1:\n",
        "        # Return the content after the query position\n",
        "        return response_text[query_position + len(query):].strip()\n",
        "    else:\n",
        "        # If the query is not found, return the full response text as a fallback\n",
        "        return response_text.strip()"
      ],
      "metadata": {
        "id": "Ap2RD8Q3MMF4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final run 1min"
      ],
      "metadata": {
        "id": "lezX38tYmBPJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can just run this cell and change the query if the setup has already run once\n",
        "\n",
        "You'll see an original which should have a link at the end that maybe or may not lead to a real database page as well as a second link which is a direct copy from the database."
      ],
      "metadata": {
        "id": "uprV8dDTr92h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How much will the state pay for childrens allowance?\"\n",
        "result = process_query(query)\n",
        "\n",
        "# Extract only the content after the query\n",
        "content_after_query = get_content_after_query(result[\"response\"], query)\n",
        "\n",
        "# Display the cleaned response and the most relevant link\n",
        "print(\"Query: \", query)\n",
        "print(\"Response:\", content_after_query)\n",
        "print(\"Most Relevant Link:\", result[\"most_relevant_link\"])"
      ],
      "metadata": {
        "id": "Ilqf1UDBMOKX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}